---
title: Parametric learning curve models for surgery training research
authors:
  - name: Martin Schmettow
    affiliation: University of Twente
    roles: writing
    corresponding: true
  - name: Marleen Groenier
    affiliation: University of Twente
    roles: writing
    corresponding: false
bibliography: references.bib
---

# Introduction

Training processes in medical care, and in particular in technically demanding domains such as minimally invasive surgery, are commonly described using the metaphor of the *learning curve*. The appeal of this metaphor is obvious: performance improves with training, improvement is initially rapid, and later gains become progressively smaller. Despite its intuitive value, however, the learning curve is often treated as a descriptive object rather than as a generative model of learning. As a result, much of the existing literature relies on loosely specified curves, endpoint comparisons, or linearized summaries that obscure the underlying mechanisms of skill acquisition and limit the interpretability of estimated effects.

This paper argues that modelling training processes requires a shift from descriptive learning curves to *parametric learning models* whose parameters have direct real-world interpretations. In particular, we focus on performance measures that decrease with skill acquisition, such as time on task, error counts, or motion inefficiency, and propose a simple three-parameter model that decomposes observed performance trajectories into interpretable components of learning.

## Problems in Training Research

A parametric perspective on learning is motivated by three recurring research problems in medical training research.

The first question concerns **selection**: can we identify trainees with high long-term potential early in training? In surgical education, this question is often framed in terms of aptitude testing, simulator performance, or early learning speed. From a modelling perspective, this corresponds to predicting an individual’s *maximum achievable performance*, that is, the asymptotic level of performance once training effects are exhausted. A model that does not contain an explicit asymptote cannot address this question in a principled way.

The second question concerns **training efficiency**: how can different training methods, curricula, or feedback regimes be compared in terms of how quickly they lead to improvement? This question maps naturally onto a *learning rate* parameter that quantifies how rapidly performance approaches its asymptote. Without an explicit rate parameter, efficiency is often approximated using arbitrary endpoints or time-to-criterion measures, both of which conflate learning dynamics with prior experience and measurement noise.

The third question concerns **transfer and preparation**: which preparatory activities lead to effective downstream learning? Examples include box trainers, virtual reality simulators, or simplified tasks such as practicing surgical maneuvers on fruit or synthetic materials. Here, the central interest lies in *previous experience*—how much effective training time a learner brings into a new task before formal training begins. From a modelling standpoint, this corresponds to a horizontal shift of the learning trajectory rather than a change in its shape.

A unifying challenge across these three questions is that they cannot be answered independently unless the learning process is decomposed into separable, interpretable components.

## Limitations of prevailing approaches

A brief look at the learning-curve literature in medical education reveals a striking pattern. While the term *learning curve* is ubiquitous, the majority of empirical studies rely on linear or piecewise linear analyses, often focusing on performance at a predefined endpoint (e.g., after a fixed number of cases) or on the slope of a regression line fitted to early training data. In many cases, improvement is summarized by subtracting early from late performance, or by testing whether a slope differs from zero.

Such approaches have two fundamental limitations.

First, **previous experience and learning rate are inseparable** in linear endpoint analyses. A trainee who starts at a better performance level due to prior exposure or positive transfer will often appear to “learn faster,” even if the underlying learning dynamics are identical. Conversely, a genuinely low learning rate cannot be distinguished from a advanced learner, already approaching the asmptote. Without an explicit model that accounts for horizontal shifts in training time, these effects are confounded by design.

Second, **maximum performance cannot be predicted at all**. Linear models either imply unbounded improvement or require an arbitrary stopping rule. Endpoint analyses implicitly assume that the observed endpoint is informative about ultimate performance, an assumption that is rarely justified, especially in long training trajectories where asymptotic performance may lie far beyond the observation window.

These limitations are not merely technical; they directly constrain the kinds of substantive questions that can be addressed. As long as learning curves are treated as descriptive trends rather than as parameterized processes, claims about aptitude, efficiency, or transfer remain underdetermined.

## A macro theory of skill acquisition

The proposed learning function is not merely a convenient mathematical form; it arises naturally from a simple generative account of how skills are built. We refer to this account as the *tweak-finder model*.

The model starts from three assumptions. First, complex skills consist of a finite set of possible refinements or *tweaks* to an initial action plan. The initial condition is a pool of undiscovered tweaks and progress in learning is ascribed as discovering these tweaks, which improves performance.
Second, discovering a tweak is effectively irreversible: once a refinement is found and incorporated, it remains available in future executions. 
Third, each undiscovered tweak has a fixed probability of being found on any given training trial. While probabilities may differ across tweaks, their average remains stable across the learning process.

Under these assumptions, skill acquisition is not a linear accumulation of improvement but a stochastic search process in a diminishing space of possibilities. Early in training, many tweaks are in the pool, and performance improves rapidly. As training progresses, fewer tweaks remain, and improvements become rarer. Because the set is finite, The resulting trajectory is necessarily asymptotic.

Formally, if the probability that a given tweak survives (i.e., remains undiscovered) from one trial to the next is constant, the expected number of remaining tweaks declines exponentially with training time. Performance, in turn, can be expressed as the sum of a non-trainable component and a trainable component proportional to the number of remaining tweaks. Collapsing the unknown number of initial tweaks and their average effect yields a three-parameter exponential model with a clear psychological interpretation.


## A parametric model of continuous learning

To address these issues, we introduce a simple three-parameter model for performance measures that decline with improvement, as is most often the case. Let performance at training time ( t ) be given by

$$
\text{perf}_t = (1 - \text{learning_rate})^{(t + \text{previous_experience})} + \text{max_performance}.
$$
with $0 \leq \text{learning_rate} \leq  1$ and $\text{max_performance} \geq 0$.

This formulation separates three conceptually distinct components of learning.

The parameter **learning_rate** determines how rapidly performance improves with training. It governs the curvature of the learning trajectory and directly corresponds to training efficiency.

The parameter **previous_experience** shifts the learning curve along the time axis, representing effective training that occurred prior to observation. Preparatory exercises, informal practice, or transferable skills are captured as prior exposure measured in units of training time.

The parameter **max_performance** represents the asymptotic lower bound of performance: the level that cannot be surpassed even with unlimited training. It formalizes the notion of a non-trainable component of task execution, arising from anatomical, physiological, or system-level constraints.

Crucially, all three parameters are interpretable on their natural scales. Changes in any one of them have qualitatively distinct effects on the learning trajectory—effects that can be visualized and empirically tested. In the remainder of the paper, we illustrate these effects using simulated and empirical data.

## Outlook

By grounding learning curves in a parametric, generative framework, we can align statistical estimation with substantive questions about selection, efficiency, and transfer. The remainder of this paper develops this approach in detail, illustrates the effects of each parameter, and discusses implications for the design and evaluation of medical training programs.

# Simulation study

To illustrate the model, we simulate a training scenario with three trainees (Li, Wang, Smith) training on a laparoscopic lacing task over 50 trials. All trainees have never performed or assisted during a laparascopy task. Yet, each trainee has different characteristics in terms of learning rate, previous experience, and asymptotic performance. Li ... Wang ... Smith.  

```{r}
library(tidyverse)
library(tidytable)
set.seed(43)


Trainees <- 
  tribble(~Trainee, ~rate, ~pexp, ~asym,       ~length,
          "Li",      0.10,      23,     0.3,       30,       # has already observed procedures
          "Smith",   0.05,       7,     0.2,        50,       # little prev exp, slow, high potential
          "Wang",    0.10,      11,     0.35,        40) |>    # fast learner, less previous experience
  arrange(Trainee)

```

```{r}
N_proc <- 100
lc <- function (t, rate, pexp, asym) {
  (1 - rate)^(t + pexp) + asym
}

D_Sim <- 
  expand_grid(Trainee = Trainees$Trainee, trial = (0:N_proc) - max(Trainees$pexp)) |> 
  left_join(Trainees) |> 
  mutate(observed = trial > 0 & trial <= length +1) |> 
  mutate(Obs = row_number()) |> 
  mutate(mu = (1 - rate)^(trial + pexp) + asym,
         duration = rnorm(n(), mu, 1/8 * mu)) |> 
  mutate(duration = if_else(observed, duration, NA_real_)) |> 
  select(Obs, Trainee, trial, mu, duration)

D_Sim %>% 
  ggplot(aes(x = trial, color = Trainee)) +
  geom_smooth(aes(y = mu), se = F) +
  geom_point(aes(y= duration), size = 1)
```
For the exponential learning curve model it is essential to start from individual-level parameters. Averaging across individuals (and other conditions) is not allowed, because sums and exponentials do not commute. Therefore, all simulations and model estimations are done on the lowest level of observation, which is the individual on a certain task.

For the modelling task it is beneficial to parametrize the model in such a way that all parameters are unbounded on the real line. This makes it easier to formulate multi-level models and priors. while maximum performance and previous experience are bounded to $R⁺$, learning rate is bound by the interval $[0; 1]$. For the zero bounded parameters we use the exponential function to stretch the interval to $R$, whereas the inverse logit function (also known as the logistic probability mass function) is applied to stretch the unit interval to the entire real line. This is very much alike to the link function in Generalized Linear Models. As the learning rate parameter no longer represents a rate, we rename it to *learning_efficiency*.

$$
\text{perf}_t = (1 - \text{logit}^{-1}\text{learning_efficiency})^{t + \exp \text{previous_experience}} + \exp \text{max_performance}.\\ 
\text{learning_efficiency} \in R
$$

```{r}
library(brms)


# priors are on log level!
F_prior <- c(set_prior("normal(-1, 5)", nlpar = "leff"),
             set_prior("normal(1, 5)", nlpar = "pexp"),
             set_prior("normal(1, 5)", nlpar = "maxp"))

F_lc <- bf(
  duration ~ (1 - inv_logit(leff))^(trial + exp(pexp)) + exp(maxp),
  leff ~ 1 + (1 | Trainee),
  pexp ~ 1 + (1 | Trainee),
  maxp ~ 1 + (1 | Trainee),
  nl = T
)

```

## Real data

```{r}
load("LapTrain.Rda")
attach(LapTrain)
D_Lap <<- D_LT |>   
  rename(duration = ToT, Trainee = Part) |>   
  mutate(trial = as.integer(trial))
detach(LapTrain)
```
```{r}
D_Lap 
```

```{r}
D_Lap |> 
  ggplot(aes(x = trial, y = duration)) +
  geom_point() +
  geom_smooth(aes(group = Trainee), se = F) +
  
```



```{r eval = F}
M_2 <- brm(
  F_lc,
  data = D_Lap,
  prior = F_prior,
  chains = 6,
  cores = 6,
  family = lognormal(link = identity),
  #iter = 5000 ,
  #warmup = 3000,
  control = list(adapt_delta = 0.99, max_treedepth = 15)
)
save(M_2, D_Lap, file = "M_2.rda")

```


```{r}
library(tidyverse)
library(bayr)
load("M_2.rda")

PP_2 <- post_pred(M_2)

predict(PP_2) |> 
  left_join(D_Lap, by = "Obs") |>
  ggplot(aes(x = trial, y = center, ymin = lower, ymax = upper)) +
  geom_line(aes(group = Trainee)) +
  geom_jitter(aes(y = duration)) +
  scale_x_discrete()

  

```
```{r}
T_1 |> 
  ggplot(aes(x = trial, y = estimate__)) +
  facet_wrap(~Trainee) +
  geom_point(aes(y = duration)) +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = Trainee), alpha = 0.2, color = NA) +
  geom_line(aes(group = Trainee))
```



```{r}
bind_rows(fixef(M_2), fixef(M_3)) |> 
  mutate(model = if_else(row_number() <= 3, "M_2", "M_3")) |>
  ggplot(aes(x = nonlin, y = center, ymin = lower, ymax = upper, color = model)) +
  geom_pointrange(position = position_dodge(width = .5))
```



```{r}
library(bayr)

ranef(M_2) |> 
  ggplot(aes(x = nonlin, y = exp(center), ymin = exp(lower), ymax = exp(upper))) +
  geom_pointrange(position = position_dodge(width = .5)) +
  scale_y_log10()

```




# A Laparascopy training experiment



```{r eval = F}
load("LapTrain.Rda")
attach(LapTrain)
D_Lap <<- D_LT |> rename(duration = ToT, Trainee = Part)
detach(LapTrain)

save(D_Lap, file = "D_Lap.rda")
```

```{r}
library(tidyverse)

D_Lap |> 
  ggplot(aes(x = trial, y = duration, color = Trainee)) +
  geom_point() +
  geom_smooth(se = F)
```

```{r}
D_Lap |> 
  ggplot(aes(x = trial, y = duration)) +
  facet_wrap(~Trainee) +
  geom_point() +
  geom_smooth(se = F)
```


```{r}
load("D_Lap.rda")

M_2 <- brm(
  F_lc,
  data = D_Lap,
  prior = F_prior,
  family = lognormal(link = identity),
  chains = 6,
  cores = 6,
  #iter = 5000 ,
  #warmup = 3000,
  control = list(adapt_delta = 0.99, max_treedepth = 15)
)

save(M_2, D_2, file = "M_2.rda")
```

## Population level effects

```{r}
fixef(M_2)
```


```{r}
grpef(M_2)
```

```{r}
fixef(M_2) |>  
  left_join(grpef(M_2), 
            by = "nonlin", suffix = c("_pop", "_sd")) |> 
  mutate(mu = exp(center_pop),
         mu = if_else(nonlin == "leff", mu/(mu + 1), mu)) |> 
  select(nonlin, mu, 
         center_pop:upper_pop, 
         center_sd:upper_sd) |> 
  knitr::kable()

```




## Trainee level effects


```{r}
ranef(M_2) |> 
  ggplot(aes(x = nonlin)) +
  geom_pointrange(aes(y = center, ymin = lower, ymax = upper, group = re_entity),
                  position = position_dodge(width = .5))
```

## Predictions 

```{r}
PP_1 <- post_pred(M_2)
```
```{r}
D_2 |>
  left_join(predict(PP_1), by = "Obs") |> 
  ggplot(aes(x = trial, y = value, color = Trainee)) +
  geom_smooth()
```




<!--## A macro theory of skill acquisition

The model introduced in this section dates back to a classic psychological experiment on the human memory. In self-experiments Ebbinghaus memorized sets of nonsense syllables and measured the number of correctly recalled syllables over time. The result is a decay curve taking the same shape that describes radioactive decay (which was only discovered a decade later). With R the rate of decay and N_0 the number of syllables (or atoms) at time t = 0, the following function produces the expected number of remaining syllables (or atoms) y at point in time t:

$$y = N_0 (1-R)^t$$ -->
<!--Note that this formula is better known in the form y = N_0 exp(rt), which is owing to the fact that many decades ago, computing arbitrary exponential equations was expensive and was usually accomplished by using logarithmic tables, which were either coded with a natural or decimal base. In consequence, parameter r contains the rate of decay, as well as a normalizing factor, which is less intuitive.


Training is building up skills, which is the opposite of decay. By a slight change in semantics it can be described as a process of decay. If we imagine that the performance we see at one time in the process, is the sum of many small performance enhancing tricks that have been discovered at an earlier moment. If we further assume that the set of possible tricks is finite (N_0), with a constant probability to be discovered (R), and that the performance measure becomes zero, when all tricks have been found, the decay model applies in the given form. However, in the context of a discovery process (rather than disappearance), it is more convenient to interpret 
However, the outcome of a training is not measured in the number of discovered tricks, and never should we be able to identify and count all the small tricks in the set. The solution is to replace N_0 by a scaling parameter, that is more general. Amplitude A scales the exponential term by the performance gain between t_0 and t_\infty. 
P_t = A(1-R)^t
While most performance metrics fall with improving performance (e.g. time on task, damage, workload), not all of them reach zero with infinite training, most notably, time on task. As the minimum time-on-task is not precisely known upfront and will vary between persons, the solution is to use an additive Asymptote parameter to the model, which allows maximum performance to be positive:
P_t = A(1-R)^t + Y
The ARY model is flexible enough to produce learning curves as those shown in Figure X and its three parameters can be interpreted as the following features of a learning sequence:
Amplitude: amount of possible improvement at the start, positive number
…
Rate: the proportion found of remaining tricks
…
Asymptote: maximum performance, positive*/

*/

## Exponential learning curve models

Exponential processes are ubiquitous in nature. In its most purest form it can be found in the decay of radioactive atoms, where the number of decaying particles $n$ with a constant probability of decay $p$ and the number of particles at time t = 0, $a$ follows:

$$n = a(1-d)^t$$
where: $0 \leq d \leq 1 $ and $a \geq 0$ and $t \in N$.

Projected to the pool of potential tweaks, this model describes how many tweaks are initially in the pool as $a - Y$. Trainees with larger values have more potential to improve, but they also start from a low initial performance (expressed as a large number). As we assume a fixed size pool $a_T$ for a given task, the only useful interpretation of $a \lt a_T $ is that some tweaks were discovered before $t = 0$, which means it is an inverse measure of previous experience. A simple re-arrangement of the formula improves the expressiveness of the model wrt. previous experience:

$$f(t) = (1-p)^{(p + t)}$$
where: $e = \log_p(a)$.
In this form, $p$ adds a number of (virtual) training trials before the training, which is a convenient measure of previous experience.

While particle decay numbers asymptotically reach zero any skill-related outcomes, like time-on-task, drop towards a strictly positive asymptote (e.g. 3 seconds), therefore:

$$
f(t) = as^t + Y
$$

Most accounts of exponential learning curve models use the following formula:

$$f(t) = a\exp(st) + Y$$
$Y$ is the asymptote and $\beta$ the possible amount of improvement. However, by using the natural exponent function, this formula misses an important aspect of learning processes, which is the *learning rate*.



## Previous experience parameter

## Multi-level learning curves

A common mistake in learning curve modelling is to average across participants. Taking an average involves building sums and these are not well-behaved in combination with exponential functions:

A simple, but not incorrect, approach to estimating learning curves is to estimate them individually and join the results afterwards to draw conclusions on the population level. The modelling framework presented here builds on the principles of multi-level modelling, which means that individual parameters are estimated on the participant level and the population level, simultaneously, which not only makes interpretation easier, but also makes the model more robust with small data sets and against outliers. For the case of trainee selection, participant-level random effects have good psychometric properties.


```{r}
#| eval: false
Ampl ~ 1 + (1 |Part)
Rate ~ 1 + (1 |Part)
Asym ~ 1 + (1 |Part)
```

For research on performance predictors it is useful to combine the learning curve with factorial or metric predictors. For example, one could ask whether speed in a mental rotation task predicts the maximum performance a surgeon can reach in a laparoscopic task.

```{r}
#| eval: false
Asym ~ 1  + speed + (1 |Part)
```


In the one-sequence model, the parameters can be interpreted exactly as was explained in the previous section. However, multi-level effects, as well as predictors are basically linear, which implies an unbounded parameter space. As shown in Table X, all three parameters of ARY have boundaries. That is a very practical problem, because the estimation engine cannot guess that these boundaries exist and can produce impossible values, such as a negative asymptote.

To solve this problem, a technique is borrowed from the framework of Generalized Linear Models which is best ascribed as linearization functions. Wrapping the amplitude and asymptote linear terms in exponential functions ensures that only positive numbers reach the learning curve. The inverse logit function squeezes the linear term for rate into $[0;1]$, making sure it is a probability.

```{r}
#| eval: false
ToT ~ exp(A) * inv_logit(R)^trial + exp(Y)
```


Model	Formula	Applications
ARY		All participants are on the same level of training at the start. Measure overall effectiveness of training.
IRY		All participants are on the same level of training at the start. Investigating variables that influence early learning
ERY		Participants differ in previous experience or measuring retention


Parameters
Parameter	Letter	Symbol	R	Range	Interpretation
Amplitude	A	δ (delta)	ampl	[0;∞]	total training potential
initial performance	I	α(alpha)	init	[ω;∞]	initial performance
previous experience	E	ψ (psi)	pexp	[0;∞]	number of (virtual) training trials before the training
Rate	R	ρ (rho)	rate	[0;∞]	efficiency (or speed) of training
Scale	S	σ (sigma)	scale	[0;∞]	scale of performance
Asymptote	Y	ω (omega)	asym	[0;∞]	potential maximum performance
Trial		t	trial	[0,1,2,… ]
trial number
Performance		y	perf	varies	performance measure
					




